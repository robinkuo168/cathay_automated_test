# /backend/services/elasticsearch_service.py
import ssl
import os
import json
import yaml
import hashlib
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
from elasticsearch import Elasticsearch
from langchain.schema import Document
from langchain_text_splitters import RecursiveJsonSplitter, CharacterTextSplitter
from langchain_elasticsearch import ElasticsearchStore
from langchain_ibm import WatsonxEmbeddings
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from dotenv import load_dotenv
from .logger import get_logger
from fastapi import HTTPException

load_dotenv()

class ElasticsearchService:
    def __init__(self, embedding_model: str = "ibm/slate-30m-english-rtrvr-v2"):
        """
        åˆå§‹åŒ– ElasticsearchServiceã€‚

        æ­¤å»ºæ§‹å‡½å¼è² è²¬è¨­å®šæ‰€æœ‰èˆ‡ Elasticsearch äº’å‹•æ‰€éœ€çš„å…ƒä»¶ï¼ŒåŒ…æ‹¬ï¼š
        1. å¾žç’°å¢ƒè®Šæ•¸è®€å–é€£ç·šè¨­å®š (ä¸»æ©Ÿã€å¸³è™Ÿã€å¯†ç¢¼)ã€‚
        2. è§£æžæ†‘è­‰æª”æ¡ˆçš„çµ•å°è·¯å¾‘ä¸¦é€²è¡Œé©—è­‰ã€‚
        3. åˆå§‹åŒ– Elasticsearch çš„ Python å®¢æˆ¶ç«¯ã€‚
        4. åˆå§‹åŒ–ç”¨æ–¼ç”Ÿæˆå‘é‡åµŒå…¥çš„ WatsonxEmbeddings æ¨¡åž‹ã€‚
        5. åˆå§‹åŒ–ç”¨æ–¼åˆ†å‰²ä¸åŒæª”æ¡ˆé¡žåž‹ (JSON, TXT) çš„æ–‡æœ¬åˆ†å‰²å™¨ã€‚
        :param embedding_model: ç”¨æ–¼ç”Ÿæˆå‘é‡åµŒå…¥çš„ Watsonx.ai æ¨¡åž‹ IDã€‚
        :raises ValueError: å¦‚æžœ Elasticsearch çš„ç’°å¢ƒè®Šæ•¸æœªå®Œæ•´è¨­å®šã€‚
        :raises FileNotFoundError: å¦‚æžœåœ¨æŒ‡å®šçš„è·¯å¾‘æ‰¾ä¸åˆ°æ†‘è­‰æª”æ¡ˆã€‚
        """
        self.logger = get_logger(__name__)

        # å¾žç’°å¢ƒè®Šæ•¸è®€å– Elasticsearch è¨­å®š
        ES_HOST = os.getenv("ES_HOST")
        ES_PORT = int(os.getenv("ES_PORT", 31041))
        ES_USERNAME = os.getenv("ES_USERNAME")
        ES_PASSWORD = os.getenv("ES_PASSWORD")

        # 1. ç²å–æ†‘è­‰çš„ç›¸å°è·¯å¾‘
        relative_cert_path = os.getenv("ES_CERT_PATH")

        if not all([ES_HOST, ES_PORT, ES_USERNAME, ES_PASSWORD, relative_cert_path]):
            raise ValueError("Elasticsearch çš„ç’°å¢ƒè®Šæ•¸æœªå®Œæ•´è¨­å®šï¼")

        project_root = Path(__file__).parent.parent.parent

        # 3. å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„èˆ‡ç›¸å°è·¯å¾‘çµåˆï¼Œå¾—åˆ°çµ•å°è·¯å¾‘
        CERT_PATH = project_root / relative_cert_path

        self.logger.info(f"æ†‘è­‰æª”æ¡ˆçš„çµ•å°è·¯å¾‘è§£æžç‚º: {CERT_PATH}")
        if not CERT_PATH.exists():
            # åœ¨åˆå§‹åŒ–æ™‚å°±æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œæå‰å¤±æ•—
            self.logger.error(f"åš´é‡éŒ¯èª¤ï¼šåœ¨è·¯å¾‘ '{CERT_PATH}' æ‰¾ä¸åˆ° Elasticsearch æ†‘è­‰æª”æ¡ˆï¼")
            raise FileNotFoundError(f"åœ¨è·¯å¾‘ '{CERT_PATH}' æ‰¾ä¸åˆ° Elasticsearch æ†‘è­‰æª”æ¡ˆï¼")

        # Elasticsearch connection URL for ElasticsearchStore
        self.es_url = f"https://{ES_USERNAME}:{ES_PASSWORD}@{ES_HOST}:{ES_PORT}"

        # Initialize direct client for management operations
        self.client = Elasticsearch(
            hosts=[{
                "host": ES_HOST,
                "port": int(ES_PORT),
                "scheme": "https"
            }],
            basic_auth=(ES_USERNAME, ES_PASSWORD),
            ca_certs=str(CERT_PATH),  # ä½¿ç”¨çµ•å°è·¯å¾‘
            verify_certs=False
        )

        # Initialize embeddings
        params = {
            EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 200,
            EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
        }

        # å¾žç’°å¢ƒè®Šæ•¸è®€å– Watsonx.ai çš„è¨­å®š
        self.embeddings = WatsonxEmbeddings(
            model_id=embedding_model,
            url=os.getenv("WATSONX_URL", "https://us-south.ml.cloud.ibm.com"),
            apikey=os.getenv("WATSONX_API_KEY"),
            project_id=os.getenv("WATSONX_PROJECT_ID"),
            params=params
        )

        # Initialize text splitters
        self.json_splitter = RecursiveJsonSplitter(max_chunk_size=300)
        self.text_splitter = CharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=75,
            separator="["
        )

        # Store ElasticsearchStore instances
        self.vector_stores = {}

    def test_connection(self) -> bool:
        """
        æ¸¬è©¦èˆ‡ Elasticsearch æœå‹™çš„é€£ç·šæ˜¯å¦æ­£å¸¸ã€‚

        :return: å¦‚æžœé€£ç·šæˆåŠŸï¼Œè¿”å›ž Trueã€‚
        :raises Exception: å¦‚æžœé€£ç·šå¤±æ•—ï¼Œå‰‡æœƒæ‹‹å‡ºåº•å±¤çš„é€£ç·šéŒ¯èª¤ã€‚
        """
        try:
            info = self.client.info()
            self.logger.info(f"Connected to Elasticsearch: {info['version']['number']}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect to Elasticsearch: {e}")
            raise e

    def get_vector_store(self, index_name: str) -> ElasticsearchStore:
        """
        ç²å–æˆ–å‰µå»ºä¸€å€‹èˆ‡ç‰¹å®šç´¢å¼•å°æ‡‰çš„ ElasticsearchStore å¯¦ä¾‹ã€‚

        æ­¤å‡½å¼ä½¿ç”¨å…§éƒ¨å¿«å– (`self.vector_stores`) ä¾†é¿å…é‡è¤‡å‰µå»ºç›¸åŒçš„
        ElasticsearchStore ç‰©ä»¶ï¼Œå¾žè€Œæé«˜æ•ˆçŽ‡ã€‚
        :param index_name: ç›®æ¨™ Elasticsearch ç´¢å¼•çš„åç¨±ã€‚
        :return: ä¸€å€‹å¯ç”¨æ–¼å‘é‡æ“ä½œçš„ ElasticsearchStore å¯¦ä¾‹ã€‚
        """
        if index_name not in self.vector_stores:
            self.vector_stores[index_name] = ElasticsearchStore(
                index_name=index_name,
                embedding=self.embeddings,
                es_connection=self.client
            )
        return self.vector_stores[index_name]

    def delete_all_documents(self, index_name: str) -> bool:
        """
        åˆªé™¤æŒ‡å®šç´¢å¼•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚

        :param index_name: è¦æ¸…ç©ºçš„ç›®æ¨™ç´¢å¼•åç¨±ã€‚
        :return: å¦‚æžœæ“ä½œæˆåŠŸï¼Œè¿”å›ž Trueï¼Œå¦å‰‡è¿”å›ž Falseã€‚
        """
        try:
            response = self.client.delete_by_query(
                index=index_name,
                body={"query": {"match_all": {}}}
            )
            self.logger.info(f"ðŸ—‘ï¸  Deleted {response['deleted']} documents from {index_name}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to delete documents from {index_name}: {e}")
            return False

    def process_json_file(self, file_path: str) -> List[Document]:
        """
        è™•ç† JSON (.json) æª”æ¡ˆï¼Œä¸»è¦ç”¨æ–¼ Langflow Agent ç‰ˆæœ¬æ–‡ä»¶ã€‚

        å°æ–¼ my_agent_versions ç´¢å¼•ï¼Œæˆ‘å€‘å°‡æ•´å€‹ JSON ä½œç‚ºå–®ä¸€æ–‡ä»¶å„²å­˜ï¼Œ
        ä¸é€²è¡Œåˆ†å‰²ï¼Œä»¥ä¿æŒ Agent é…ç½®çš„å®Œæ•´æ€§ã€‚
        :param file_path: JSON æª”æ¡ˆçš„è·¯å¾‘ã€‚
        :return: ä¸€å€‹åŒ…å«å¾žæª”æ¡ˆä¸­æå–å‡ºçš„ Document ç‰©ä»¶çš„åˆ—è¡¨ã€‚
        :raises Exception: å¦‚æžœåœ¨è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚
        """
        documents = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                json_data = json.load(file)

            # For agent versions, store ONLY the complete JSON as a single document
            # Do NOT create chunked versions to avoid multiple documents
            full_doc = Document(
                page_content=json.dumps(json_data, ensure_ascii=False, indent=2),
                metadata={
                    "file_path": str(file_path),
                    "filetype": "This is a JSON/.json file (Agent Version)",
                    "is_full_document": True,
                    "source": str(file_path),
                    "agent_version": json_data.get("version", "unknown"),
                    "agent_name": json_data.get("name", "unknown"),
                    "created_at": json_data.get("created_at", "unknown")
                }
            )
            documents.append(full_doc)

            # REMOVED: Chunked versions creation to keep only 1 document per agent
            # This eliminates the 4 additional chunk documents

        except Exception as e:
            self.logger.error(f"Error processing JSON file {file_path}: {e}")
            raise
        return documents

    def process_xlsx_file(self, file_path: str) -> List[Document]:
        """
        è™•ç† Excel (.xlsx) æª”æ¡ˆï¼Œä¸¦å°‡å…¶å…§å®¹è½‰æ›ç‚º LangChain çš„ Document ç‰©ä»¶åˆ—è¡¨ã€‚

        æ­¤å‡½å¼æœƒéæ­· Excel ä¸­çš„æ¯ä¸€å€‹å·¥ä½œè¡¨ (sheet)ï¼Œå°‡æ¯ä¸€è¡Œè½‰æ›ç‚ºä¸€å€‹ç¨ç«‹çš„ Documentï¼Œ
        åŒæ™‚ä¹Ÿæœƒç‚ºæ•´å€‹å·¥ä½œè¡¨å‰µå»ºä¸€å€‹åŒ…å«æ‰€æœ‰å…§å®¹çš„ Documentï¼Œä»¥æ”¯æ´ä¸åŒç²’åº¦çš„æª¢ç´¢ã€‚
        :param file_path: Excel æª”æ¡ˆçš„è·¯å¾‘ã€‚
        :return: ä¸€å€‹åŒ…å«å¾žæª”æ¡ˆä¸­æå–å‡ºçš„æ‰€æœ‰ Document çš„åˆ—è¡¨ã€‚
        :raises Exception: å¦‚æžœåœ¨è®€å–æˆ–è™•ç† Excel æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚
        """
        documents = []
        try:
            excel_file = pd.ExcelFile(file_path)
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
                for col in df.select_dtypes(include=['datetime64[ns]']).columns:
                    df[col] = df[col].astype(str)
                for col in df.select_dtypes(include=['category']).columns:
                    df[col] = df[col].astype(str)
                df = df.where(pd.notnull(df), None)
                for col in df.select_dtypes(include=[np.number]).columns:
                    df[col] = df[col].apply(lambda x: x.item() if isinstance(x, np.generic) else x)
                excel_data = df.to_dict(orient='records')
                for i, row_data in enumerate(excel_data):
                    doc = Document(
                        page_content=json.dumps(row_data, ensure_ascii=False),
                        metadata={
                            "file_path": f"{sheet_name}#{file_path}",
                            "filetype": "This is a Excel/.xlsx file",
                            "chunk_index": i,
                            "sheet_name": sheet_name,
                            "source": str(file_path)
                        }
                    )
                    documents.append(doc)
                full_doc = Document(
                    page_content=json.dumps(excel_data, ensure_ascii=False),
                    metadata={
                        "file_path": f"{sheet_name}#{file_path}#full",
                        "filetype": "This is a Excel/.xlsx file",
                        "sheet_name": sheet_name,
                        "is_full_document": True,
                        "source": str(file_path)
                    }
                )
                documents.append(full_doc)
        except Exception as e:
            self.logger.error(f"Error processing Excel file {file_path}: {e}")
            raise
        return documents

    def process_txt_file(self, file_path: str) -> List[Document]:
        """
        è™•ç†ç´”æ–‡å­— (.txt) æª”æ¡ˆï¼Œå°‡å…¶åˆ†å‰²æˆå¡Š (chunks)ï¼Œä¸¦è½‰æ›ç‚º Document ç‰©ä»¶åˆ—è¡¨ã€‚

        :param file_path: ç´”æ–‡å­—æª”æ¡ˆçš„è·¯å¾‘ã€‚
        :return: ä¸€å€‹åŒ…å«å¾žæª”æ¡ˆä¸­æå–ä¸¦åˆ†å‰²çš„æ‰€æœ‰ Document çš„åˆ—è¡¨ã€‚
        :raises Exception: å¦‚æžœåœ¨è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚
        """
        documents = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            text_chunks = self.text_splitter.split_text(content)
            for i, chunk in enumerate(text_chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "file_path": str(file_path),
                        "filetype": "This is a TEXT/.txt file",
                        "chunk_index": i,
                        "source": str(file_path)
                    }
                )
                documents.append(doc)
        except Exception as e:
            self.logger.error(f"Error processing text file {file_path}: {e}")
            raise
        return documents

    def process_yaml_file(self, file_path: str) -> List[Document]:
        """
        è™•ç† YAML (.yaml) æª”æ¡ˆï¼Œå°‡å…¶å…§å®¹åˆ†å‰²æˆå¡Šï¼Œä¸¦è½‰æ›ç‚º Document ç‰©ä»¶åˆ—è¡¨ã€‚

        :param file_path: YAML æª”æ¡ˆçš„è·¯å¾‘ã€‚
        :return: ä¸€å€‹åŒ…å«å¾žæª”æ¡ˆä¸­æå–ä¸¦åˆ†å‰²çš„æ‰€æœ‰ Document çš„åˆ—è¡¨ã€‚
        :raises Exception: å¦‚æžœåœ¨è®€å–æˆ–è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚
        """
        documents = []
        try:
            with open(file_path, 'rb') as file:
                yaml_data = yaml.safe_load(file)
            yaml_chunks = self.json_splitter.split_json(json_data=yaml_data)
            for i, chunk in enumerate(yaml_chunks):
                doc = Document(
                    page_content=json.dumps(chunk, ensure_ascii=False),
                    metadata={
                        "file_path": str(file_path),
                        "filetype": "This is a YAML/.yaml file",
                        "chunk_index": i,
                        "source": str(file_path)
                    }
                )
                documents.append(doc)
            full_doc = Document(
                page_content=json.dumps(yaml_data, ensure_ascii=False),
                metadata={
                    "file_path": f"{str(file_path)}#full",
                    "filetype": "This is a YAML/.yaml file",
                    "is_full_document": True,
                    "source": str(file_path)
                }
            )
            documents.append(full_doc)
        except Exception as e:
            self.logger.error(f"Error processing YAML file {file_path}: {e}")
            raise
        return documents

    def process_file(self, file_path: str) -> List[Document]:
        """
        æ ¹æ“šæª”æ¡ˆçš„å‰¯æª”åï¼Œå‹•æ…‹åœ°é¸æ“‡åˆé©çš„è™•ç†å‡½å¼ä¾†è™•ç†å–®ä¸€æª”æ¡ˆã€‚

        é€™æ˜¯ä¸€å€‹èª¿åº¦å‡½å¼ (dispatcher)ï¼Œå®ƒæœƒæ ¹æ“šå‰¯æª”åå‘¼å«å°æ‡‰çš„
        `process_xlsx_file`, `process_txt_file`, `process_yaml_file` æˆ– `process_json_file`ã€‚
        :param file_path: è¦è™•ç†çš„æª”æ¡ˆè·¯å¾‘ã€‚
        :return: ä¸€å€‹å¾žæª”æ¡ˆä¸­æå–å‡ºçš„ Document ç‰©ä»¶åˆ—è¡¨ã€‚
        :raises ValueError: å¦‚æžœæª”æ¡ˆé¡žåž‹ä¸è¢«æ”¯æ´ã€‚
        """
        file_path = Path(file_path)
        extension = file_path.suffix.lower()
        if extension in ['.xlsx', '.xls']:
            return self.process_xlsx_file(str(file_path))
        elif extension == '.txt':
            return self.process_txt_file(str(file_path))
        elif extension in ['.yaml', '.yml']:
            return self.process_yaml_file(str(file_path))
        elif extension == '.json':
            return self.process_json_file(str(file_path))
        else:
            raise ValueError(f"Unsupported file type: {extension}")

    def check_document_exists(self, document: Document, index_name: str) -> bool:
        """
        æª¢æŸ¥ä¸€å€‹ç‰¹å®šçš„ Document ç‰©ä»¶æ˜¯å¦å·²ç¶“å­˜åœ¨æ–¼æŒ‡å®šçš„ç´¢å¼•ä¸­ï¼Œä»¥é¿å…é‡è¤‡ä¸Šå‚³ã€‚

        å®ƒé€šéŽå°æ–‡ä»¶å…§å®¹å’Œå…ƒæ•¸æ“šé€²è¡Œé›œæ¹Š (hash) ä¾†ç”Ÿæˆä¸€å€‹å”¯ä¸€çš„æ¨™è­˜ç¬¦ï¼Œ
        ä¸¦åœ¨ Elasticsearch ä¸­æŸ¥è©¢è©²æ¨™è­˜ç¬¦ã€‚
        :param document: è¦æª¢æŸ¥çš„ LangChain Document ç‰©ä»¶ã€‚
        :param index_name: ç›®æ¨™ Elasticsearch ç´¢å¼•çš„åç¨±ã€‚
        :return: å¦‚æžœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›ž Trueï¼Œå¦å‰‡è¿”å›ž Falseã€‚
        """
        try:
            content_hash = hashlib.md5(
                (document.page_content + str(document.metadata.get("file_path", ""))).encode()
            ).hexdigest()
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"metadata.file_path.keyword": document.metadata.get("file_path", "")}},
                            {"term": {"metadata.chunk_index": document.metadata.get("chunk_index", -1)}}
                        ]
                    }
                },
                "size": 1
            }
            response = self.client.search(index=index_name, body=search_body)
            return response["hits"]["total"]["value"] > 0
        except Exception:
            return False

    def upload_documents(self, documents: List[Document], index_name: str, check_duplicates: bool = True) -> bool:
        """
        å°‡ä¸€å€‹ Document ç‰©ä»¶åˆ—è¡¨ä¸Šå‚³è‡³ Elasticsearchï¼Œä¸¦å¯é¸æ“‡æ€§åœ°é€²è¡Œæ‰¹æ¬¡é‡è¤‡æª¢æŸ¥ã€‚

        å¦‚æžœå•Ÿç”¨é‡è¤‡æª¢æŸ¥ï¼Œå®ƒæœƒä½¿ç”¨é«˜æ•ˆçš„ `mget` æ“ä½œä¸€æ¬¡æ€§æª¢æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œ
        ç„¶å¾Œåªä¸Šå‚³æ–°çš„æ–‡ä»¶ï¼Œå¤§å¹…æå‡äº†é‡è¤‡ä¸Šå‚³æ™‚çš„æ•ˆçŽ‡ã€‚
        :param documents: è¦ä¸Šå‚³çš„ Document ç‰©ä»¶åˆ—è¡¨ã€‚
        :param index_name: ç›®æ¨™ Elasticsearch ç´¢å¼•çš„åç¨±ã€‚
        :param check_duplicates: æ˜¯å¦åœ¨ä¸Šä¼ å‰æª¢æŸ¥é‡è¤‡ã€‚
        :return: å¦‚æžœæ“ä½œæˆåŠŸï¼Œè¿”å›ž Trueï¼Œå¦å‰‡è¿”å›ž Falseã€‚
        """
        try:
            vector_store = self.get_vector_store(index_name)
            if check_duplicates:
                doc_ids = []
                for doc in documents:
                    file_path = doc.metadata.get("file_path", "")
                    content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
                    combined_key = f"{file_path}:{content_hash}"
                    doc_id = hashlib.md5(combined_key.encode()).hexdigest()
                    doc_ids.append(doc_id)
                try:
                    response = self.client.mget(
                        index=index_name,
                        body={"ids": doc_ids},
                        _source=False
                    )
                    existing_ids = {doc_response["_id"] for doc_response in response["docs"] if
                                    doc_response.get("found", False)}
                    new_documents = [doc for doc, doc_id in zip(documents, doc_ids) if doc_id not in existing_ids]
                    new_doc_ids = [doc_id for doc_id in doc_ids if doc_id not in existing_ids]
                    if new_documents:
                        vector_store.add_documents(new_documents, ids=new_doc_ids)
                        self.logger.info(
                            f"Added {len(new_documents)} new documents (skipped {len(existing_ids)} existing)")
                        return True
                    else:
                        self.logger.info("â„¹ï¸  No new documents to add - all documents already exist")
                        return True
                except Exception as e:
                    self.logger.warning(
                        f"Index '{index_name}' doesn't exist yet or mget failed, adding all documents. Error: {e}")
                    vector_store.add_documents(documents, ids=doc_ids)
                    return True
            else:
                vector_store.add_documents(documents)
                self.logger.info(f"Added {len(documents)} documents to index")
                return True
        except Exception as e:
            self.logger.error(f"Failed to upload documents: {e}")
            return False

    def upload_file(self, file_path: str, index_name: str, check_duplicates: bool = True) -> bool:
        """
        ä¸€å€‹æ–¹ä¾¿çš„åŒ…è£å‡½å¼ï¼Œç”¨æ–¼è™•ç†ä¸¦ä¸Šå‚³å–®ä¸€æª”æ¡ˆã€‚

        å®ƒæœƒå…ˆå‘¼å« `process_file` å°‡æª”æ¡ˆè½‰æ›ç‚º Document åˆ—è¡¨ï¼Œç„¶å¾Œå†å‘¼å« `upload_documents` é€²è¡Œä¸Šå‚³ã€‚
        :param file_path: è¦ä¸Šå‚³çš„æª”æ¡ˆè·¯å¾‘ã€‚
        :param index_name: ç›®æ¨™ Elasticsearch ç´¢å¼•çš„åç¨±ã€‚
        :param check_duplicates: æ˜¯å¦åœ¨ä¸Šä¼ å‰æª¢æŸ¥é‡è¤‡ã€‚
        :return: å¦‚æžœæ“ä½œæˆåŠŸï¼Œè¿”å›ž Trueï¼Œå¦å‰‡è¿”å›ž Falseã€‚
        """
        try:
            self.logger.info(f"ðŸ“„ Processing file: {file_path}")
            documents = self.process_file(file_path)
            if documents:
                return self.upload_documents(documents, index_name, check_duplicates)
            else:
                self.logger.warning(f"No documents generated from {file_path}")
                return True
        except Exception as e:
            self.logger.error(f"Failed to upload file {file_path}: {e}")
            return False

    def upload_multiple_files(self, file_paths: List[str], index_name: str,
                              delete_existing: bool = False, check_duplicates: bool = True) -> bool:
        """
        ä¸Šå‚³å¤šå€‹æª”æ¡ˆè‡³ Elasticsearch çš„ä¸»è¦é€²å…¥é»žã€‚

        æ­¤å‡½å¼å”èª¿æ•´å€‹ä¸Šå‚³æµç¨‹ï¼ŒåŒ…æ‹¬æ¸¬è©¦é€£ç·šã€å¯é¸åœ°åˆªé™¤èˆŠç´¢å¼•ï¼Œ
        ä»¥åŠè¿­ä»£è™•ç†æ¯ä¸€å€‹æª”æ¡ˆã€‚
        :param file_paths: ä¸€å€‹åŒ…å«å¤šå€‹æª”æ¡ˆè·¯å¾‘çš„åˆ—è¡¨ã€‚
        :param index_name: ç›®æ¨™ Elasticsearch ç´¢å¼•çš„åç¨±ã€‚
        :param delete_existing: æ˜¯å¦åœ¨ä¸Šä¼ å‰åˆªé™¤å·²å­˜åœ¨çš„åŒåç´¢å¼•ã€‚
        :param check_duplicates: æ˜¯å¦åœ¨ä¸Šä¼ æ¯å€‹æª”æ¡ˆæ™‚æª¢æŸ¥é‡è¤‡ã€‚
        :return: å¦‚æžœæ‰€æœ‰æª”æ¡ˆéƒ½æˆåŠŸè™•ç†ï¼Œè¿”å›ž Trueï¼Œå¦å‰‡è¿”å›ž Falseã€‚
        """
        try:
            if not self.test_connection():
                return False
            if delete_existing:
                self.delete_all_documents(index_name)
            success_count = 0
            total_files = len(file_paths)
            for i, file_path in enumerate(file_paths):
                self.logger.info(f"ðŸ“ Processing file {i + 1}/{total_files}: {file_path}")
                if self.upload_file(file_path, index_name, check_duplicates):
                    success_count += 1
                else:
                    self.logger.error(f"Failed to process: {file_path}")
            self.logger.info(f"ðŸŽ‰ Upload completed! {success_count}/{total_files} files processed successfully.")
            try:
                stats = self.client.count(index=index_name)
                self.logger.info(f"Total documents in index '{index_name}': {stats['count']}")
            except Exception as e:
                self.logger.warning(f"Could not retrieve index stats: {e}")
            return success_count == total_files
        except Exception as e:
            self.logger.error(f"Upload process failed: {e}")
            return False

    def search_documents(self, query: str, index_name: str, k: int = 5) -> List[Document]:
        """
        åœ¨æŒ‡å®šçš„ç´¢å¼•ä¸­ï¼Œæ ¹æ“šå‘é‡ç›¸ä¼¼åº¦åŸ·è¡Œæœå°‹ã€‚

        :param query: ä½¿ç”¨è€…çš„è‡ªç„¶èªžè¨€æŸ¥è©¢ã€‚
        :param index_name: è¦æœå°‹çš„ç›®æ¨™ç´¢å¼•åç¨±ã€‚
        :param k: è¦è¿”å›žçš„æœ€ç›¸ä¼¼çµæžœæ•¸é‡ã€‚
        :return: ä¸€å€‹åŒ…å«æœ€ç›¸ä¼¼çš„ Document ç‰©ä»¶çš„åˆ—è¡¨ã€‚
        """
        try:
            vector_store = self.get_vector_store(index_name)
            return vector_store.similarity_search(query, k=k)
        except Exception as e:
            self.logger.error(f"Search failed: {e}")
            return []

    def search_with_score(self, query: str, index_name: str, k: int = 5) -> List[tuple]:
        """
        åŸ·è¡Œå‘é‡ç›¸ä¼¼åº¦æœå°‹ï¼Œä¸¦åœ¨çµæžœä¸­åŒ…å«æ¯å€‹æ–‡ä»¶çš„ç›¸ä¼¼åº¦åˆ†æ•¸ã€‚

        :param query: ä½¿ç”¨è€…çš„è‡ªç„¶èªžè¨€æŸ¥è©¢ã€‚
        :param index_name: è¦æœå°‹çš„ç›®æ¨™ç´¢å¼•åç¨±ã€‚
        :param k: è¦è¿”å›žçš„æœ€ç›¸ä¼¼çµæžœæ•¸é‡ã€‚
        :return: ä¸€å€‹å…ƒçµ„çš„åˆ—è¡¨ï¼Œæ¯å€‹å…ƒçµ„åŒ…å« (Document, score)ã€‚
        """
        try:
            vector_store = self.get_vector_store(index_name)
            return vector_store.similarity_search_with_score(query, k=k)
        except Exception as e:
            self.logger.error(f"Search with score failed: {e}")
            return []

    async def get_agent_json(self, index_name: str = "my_agent_versions") -> Dict:
        """Retrieve the ORIGINAL JSON file from my_agent_versions index"""
        try:

            self.logger.info(f"ðŸ” Searching for documents in index: {index_name}")

            response = self.client.search(
                index=index_name,
                body={"query": {"match_all": {}}, "size": 1}
            )

            hits = response["hits"]["hits"]
            self.logger.info(f"ðŸ“Š Found {len(hits)} documents in {index_name}")

            if not hits:
                raise HTTPException(status_code=404, detail=f"No documents found in index {index_name}")

            document = hits[0]["_source"]
            self.logger.info(f"ðŸ”‘ Document keys: {list(document.keys())}")

            # The JSON is stored in 'text' field (not page_content)
            if "text" in document:
                text_content = document["text"]
                self.logger.info(f"ðŸ“ Found text field, length: {len(text_content)}")
                self.logger.info(f"ðŸ“ Text content preview: {text_content[:200]}...")

                try:
                    # Parse the JSON string back to original structure
                    original_json = json.loads(text_content)
                    self.logger.info(f"âœ… Successfully parsed text as JSON")
                    self.logger.info(
                        f"ðŸ”‘ Original JSON keys: {list(original_json.keys()) if isinstance(original_json, dict) else 'Not a dict'}")
                    return original_json

                except json.JSONDecodeError as e:
                    self.logger.error(f"âŒ Failed to parse text as JSON: {str(e)}")
                    raise HTTPException(status_code=500, detail="Stored JSON data is corrupted")
            else:
                self.logger.error(f"âŒ No 'text' field found. Available fields: {list(document.keys())}")
                raise HTTPException(status_code=500, detail="Document missing text field")

        except HTTPException:
            raise
        except Exception as e:
            self.logger.error(f"âŒ Error retrieving agent JSON from Elasticsearch: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Failed to retrieve agent JSON: {str(e)}")

    async def get_agent_json_bytes(self) -> bytes:
        """Retrieve JSON from Elasticsearch and return as bytes"""
        try:
            # Get JSON from Elasticsearch
            agent_data = await self.get_agent_json()

            # Convert to JSON string and then to bytes
            json_string = json.dumps(agent_data, indent=2, ensure_ascii=False)
            json_bytes = json_string.encode('utf-8')

            self.logger.info("Agent JSON retrieved and converted to bytes")
            return json_bytes

        except Exception as e:
            self.logger.error(f"Error converting agent JSON to bytes: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Failed to get agent JSON bytes: {str(e)}")